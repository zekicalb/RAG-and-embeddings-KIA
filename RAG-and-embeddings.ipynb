{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"machine learning algorithms\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Original Query Text'  # Replace with your actual query text if needed\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Hogwarts')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Retrieved Results to LLM\n",
    "\n",
    "- Combine learnings from week 1 with approach from this week to inject your data into prompts and create a simple question answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
